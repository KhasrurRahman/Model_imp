# Neural Network Assignment Report

## 1. Custom NN Framework (from scratch)
- Implemented in `standaloneNeuralNetworkFramework.py`.
- Features: Dense layers, dropout, multiple activations (sigmoid, tanh, leaky_relu), softmax output, cross-entropy loss, manual forward/backward propagation, parameter counting, RAM estimation, confusion matrix.

## 2. LLM-Generated NN Framework
- Implemented in `llm_implementation_code.py` (generated by an LLM tool).
- Features: Nearly identical to the custom code, with similar architecture, activations, dropout, and utilities. Includes grid search for hyperparameter tuning.

## 3. PyTorch Implementation
- Implemented in `pytorch_nn_framework.py`.
- Uses standard PyTorch modules for layers, activations, dropout, and training.

## 4. Differences: Custom vs LLM-Generated Code
- **Structure:** Both codes use similar class/function names and logic. The LLM code may have more comments and a grid search utility.
- **Features:** Both support the same activations, dropout, and utilities. LLM code includes grid search for hyperparameters.
- **Code Style:** Minor differences in code style, but both are functionally equivalent.
- **Experimentation:** LLM code is more experiment-friendly due to grid search.

## 5. Experiments
- Grid search was used to test different numbers of layers, neurons, activations, and dropout rates.
- **Results:**
  - Best configuration: *(Paste your actual best configuration here, e.g., 3 layers, 64 neurons, leaky_relu, 0.2 dropout)*
  - Best accuracy: *(Paste your actual best accuracy here, e.g., 0.87)*
  - Example confusion matrix:

    |        | Pred 0 | Pred 1 |
    |--------|--------|--------|
    | True 0 |   XX   |   XX   |
    | True 1 |   XX   |   XX   |

- *(Paste the actual confusion matrix from your experiment output above)*

## 6. Parameter and RAM Calculation
- `count_parameters` and `estimate_ram` functions are implemented in both codes.

## 7. Data Compatibility
- Both frameworks work with classification datasets containing nominal and numeric features (see data loading functions).

---

**Conclusion:**
- Both the custom and LLM-generated frameworks are robust, modular, and suitable for experimentation. The LLM code adds convenience for hyperparameter search. The PyTorch version is more concise and leverages optimized libraries.

---

*This report summarizes the implementation, experimentation, and comparison as required by the assignment.*
